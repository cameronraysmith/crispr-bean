#!/usr/bin/env python

import logging
import os
import sys
from multiprocessing import Pool

import beret
import pandas as pd
from utils import InputFileError, _get_input_parser

logging.basicConfig(
    level=logging.INFO,
    format="%(levelname)-5s @ %(asctime)s:\n\t %(message)s \n",
    datefmt="%a, %d %b %Y %H:%M:%S",
    stream=sys.stderr,
    filemode="w",
)
error = logging.critical
warn = logging.warning
debug = logging.debug
info = logging.info


def get_input_parser():
    parser = _get_input_parser()
    parser.add_argument(
        "-i",
        "--input",
        type=str,
        help="List of fastq and sample ids. Formatted as `R1_filepath,R2_filepath,sample_id`",
        required=True,
    )
    parser.add_argument(
        "-t", "--threads", type=int, help="Number of threads", default=10
    )
    parser.add_argument(
        "--guide_start_seqs_file",
        type=str,
        help="CSV file path with per-sample `guide_start_seq` to be used."
        + "Formatted as `sample_id, guide_start_seq`",
        default=None,
    )
    parser.add_argument(
        "--guide_end_seqs_file",
        type=str,
        help="CSV file path with per-sample `guide_end_seq` to be used."
        + "Formatted as `sample_id,guide_end_seq`",
        default=None,
    )
    parser.add_argument("--rerun", help="Recount each sample", action="store_true")

    return parser


def count_sample(R1, R2, sample_id, args):
    args_dict = vars(args)
    args_dict["R1"] = R1
    args_dict["R2"] = R2
    args_dict["name"] = sample_id
    args_dict["output_folder"] = os.path.join(args.output_folder, sample_id)
    base_editing_map = {"A": "G", "C": "T"}
    edited_from = args_dict["edited_base"]
    edited_to = base_editing_map[edited_from]
    match_target_pos = args_dict["match_target_pos"]
    if "guide_start_seqs_tbl" in args_dict:
        args_dict["guide_start_seq"] = args_dict["guide_start_seqs_tbl"][sample_id]
    if "guide_end_seqs_tbl" in args_dict:
        args_dict["guide_end_seq"] = args_dict["guide_end_seqs_tbl"][sample_id]
    counter = beret.mp.GuideEditCounter(**args_dict)
    if os.path.exists(f"{counter.output_dir}.h5ad") and not args_dict["rerun"]:
        screen = beret.read_h5ad(f"{counter.output_dir}.h5ad")
        if counter.count_reporter_edits and match_target_pos:
            screen.get_edit_mat_from_uns(edited_from, edited_to, match_target_pos)
        info(
            f"Reading already existing data for {sample_id} from \n\
            {counter.output_dir}.h5ad"
        )

    else:
        info(f"Counting {sample_id}")
        counter.check_filter_fastq()
        counter.get_counts()
        counter.screen.write(f"{counter.output_dir}.h5ad")
        screen = counter.screen
        if counter.count_reporter_edits and match_target_pos:
            screen.get_edit_mat_from_uns(edited_from, edited_to, match_target_pos)
        info(
            f"Done for {sample_id}. \n\
            Output written at {counter.output_dir}.h5ad"
        )

    return screen


def check_arguments(args):
    sample_tbl = pd.read_csv(args.input, header=None)
    if len(sample_tbl[2].unique()) != len(sample_tbl[2]):
        raise InputFileError(
            f"Sample ID not unique. Please check your input file {args.input}."
        )
    if args.guide_start_seqs_file is not None:
        guides_start_seqs_tbl = pd.read_csv(
            args.args.guide_start_seqs_file, header=None
        )
        sample_has_start_seq = sample_tbl[2].isin(guides_start_seqs_tbl[0])
        if not sample_has_start_seq.all():
            raise InputFileError(
                f"Sample ID(s) {sample_tbl[2][~sample_has_start_seq]} not in guides_start_seqs_file {args.guide_start_seqs_tbl}"
            )
        guides_start_seqs_tbl.columns = ["sample", "seq"]
        args.guides_start_seqs_tbl = guides_start_seqs_tbl.set_index("sample")["seq"]
    if args.guide_end_seqs_file is not None:
        guides_end_seqs_tbl = pd.read_csv(args.args.guide_end_seqs_file, header=None)
        sample_has_end_seq = sample_tbl[2].isin(guides_end_seqs_tbl[0])
        if not sample_has_end_seq.all():
            raise InputFileError(
                f"Sample ID(s) {sample_tbl[2][~sample_has_end_seq]} not in guides_end_seqs_file {args.guide_end_seqs_tbl}"
            )
        guides_end_seqs_tbl.columns = ["sample", "seq"]
        args.guides_end_seqs_tbl = guides_end_seqs_tbl.set_index("sample")["seq"]
    return args


def main():
    parser = get_input_parser()
    args = parser.parse_args()
    args = check_arguments(args)

    sample_tbl = pd.read_csv(args.input, header=None)

    p = Pool(processes=args.threads)
    result = p.starmap(
        count_sample,
        [list(tup) + [args] for tup in list(sample_tbl.to_records(index=False))],
    )
    # result = p.starmap(count_sample, sample_tbl[0], sample_tbl[1], sample_tbl[2])
    p.close()

    screen = beret.concat(result, axis=1)
    database_id = args.name or args.input.split(".")[0]
    output_path = os.path.join(
        os.path.abspath(args.output_folder), f"beret_count_{database_id}"
    )

    screen.guides = result[0].guides.loc[screen.guides.index, :]
    screen.condit = screen.condit
    screen.write(f"{output_path}.h5ad")

    info("All Done!")
    print(
        r"""
          )                                             )
         (           ________________________          (
        __)__       | __   __            ___ |        __)__
     C\|     \      |/  ` /  \ |  | |\ |  |  |     C\|     \
       \     /      |\__, \__/ \__/ | \|  |  |       \     /
        \___/       |________________________|        \___/
    """
    )


if __name__ == "__main__":
    main()
